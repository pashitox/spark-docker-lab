services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.0
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 10

  kafka:
    image: confluentinc/cp-kafka:7.0.0
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "bash", "-c", "timeout 2 bash -c '</dev/tcp/localhost/9092'"]
      interval: 30s
      timeout: 10s
      retries: 10

  kafka-setup:
    image: confluentinc/cp-kafka:7.0.0
    restart: on-failure
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - spark-net
    command: >
      bash -c "
      echo 'Waiting for Kafka to be ready...';
      sleep 10;
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --topic sensor_topic --partitions 1 --replication-factor 1;
      echo 'Topic sensor_topic created successfully';
      exit 0
      "

  spark-master:
    image: bitnami/spark:3.4.1
    restart: unless-stopped
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "bash", "-c", "timeout 2 bash -c '</dev/tcp/localhost/7077'"]
      interval: 30s
      timeout: 10s
      retries: 10

  spark-worker:
    image: bitnami/spark:3.4.1
    restart: unless-stopped
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - spark-net
    deploy:
      replicas: 2

  spark-app:
    image: pashitox/spark-docker-lab:latest
    #build:
    #  context: .
    #  dockerfile: Dockerfile
    restart: unless-stopped
    user: "1001"
    depends_on:
      kafka-setup:
        condition: service_completed_successfully
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BROKERS=kafka:9092
    networks:
      - spark-net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  spark-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16